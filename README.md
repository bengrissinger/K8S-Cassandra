# How to recover Cassandra faster

New solution archtiectures and methodologies for building, saving, and running our enterprise application stacks are having a profound impact on how we manage underlying storage and data.  Placing applications into highly repeatable stacks using microserviced containers brings massive benefits such as speed, endurance, and flexibility.  With these changes, there is a parallel importance to introduce new ideas how we manage data and storage supporting business requirements such as Disaster Recovery planning and Backups.  

In this post, our objective is to demonstrate how we can utilize PX from PortWorx to improve Recovery Time Objectives (RTO).  RTO is a key metric used to assess business application production readiness by measuring the amount of time required to recover an application to an acceptable state in the event of outages.    As application usage grows, so does the amount underlying amount of processed and stored data, thus one could expect an initial RTO measurement to increase over time.  Recovery Point Objectives (RPO) are also part of the application production readiness picture as this metric defines what are acceptable adequate recovery restore points.  Application growth can quickly begin to exceed what is able to be recovered in adequate time frames and at intervals and points in time that are acceptable. Other challenges from application growth also begin to come into view such as how to recover data generated during downtime recovery periods and the amount of time reduced number of available connections or less process resource result in degraded performance during outages.  

If your DR recovery tests are taking to long to create or recover, or they are not reaching acceptable recovery points; then your disaster recovery plans need data and storage management solutions from PortWorx.   

We chose to use Cassandra running as a stateful service on a K8S cluster to help demonstrate how we can greatly improve RPO possibiltles.  Cassandra running on a K8S cluster can be fully automated to rebuild and recover all application data including data generated during the time it takes to recover a node.   Cassandra running on a K8S cluster accomplishes recovery in two distinct stages, bootstrapping and repair.   Bootstrapping are the steps taken by K8S and Cassandra together to recognize a failure has occurred and then execute processes to join a new node and recover the data of a last known good state.   When bootstrapping finishes, repair steps are then executed, where all transactions that occurred during the bootstrap stage are also captured and written to the Cassandra recovery node as well.   This is a great RPO story, especially for highly transactional applications that need robust capabilities to recover all data that was already written to disk prior to a failure, but also including all data that was generated during the time it took to perform a recovery to a current state.  However, the ongoing growth rate of your application and data will impact RTO over time, therefore dynamically changing the amount of time it takes to recover fully failed nodes.  This scenario is one example where significant value is realized by using PX as a storage and data management solution that improves your overall recovery plan.   

For this demo, we are going to use an existing kubernetes 1.7x cluster consisting of 1 untainted master node and 3 worker hosts.  We are running a single ring Cassandra cluster using a K8S replication controller configuration service to run a 2 cassandra replicas.  This configuration allows for a single unused remaining node to be available as a failover node for our test scenarios.  

There will be two seperate tests in this demo.  Both tests will have attached dedicated storage attached to each specific host instance.  In the first test, storage will be presented and consumed as a direct local storage mount point.  In the second test, we will use the PX cli to create PX managed volumes instead of uisng the local mount points.  Portworx does provide dynamic allocation of storage, but for this demo it was decided to use static in line locally created PX volumes as we are limited to a smaller fixed cluster size and we wanted to visually display the time and steps where PX provides improvements.   We are going to capture the amount of time it takes to run a Cassandra native bootstrap and repair process to recover from a simulated failure to that of the amount of time using a Portworx PX volume snap to replace the bootstrap steps along with the repair steps.  

In the first test of this demo, we will run on an existing Kubernetes cluster a Cassandra replication controller service of 2 instances running on a 3 node K8S cluster, where each of the 3 worker nodes has locally provisioned multi path'd attached mount points created exclusively for each potential Cassandra replica instance that may run on each node.  We are going to intentionally fail one of the 2 running cassandra replica nodes, and watch how the Cassandra replica service fails over to the remaining worker node.   

In the second part of this demo, we are going to repeat much the same as the first test, a K8S Cassandra 2 instance replica service on the 3 worker node cluster, but instead we'll use pre-provisioned PX volumes on each of the worker nodes, and again we will repeat the intentional node failure scenario. In this test though, we are going to use a PX snap to "PX bootstrap" the recovery node instead of the Cassandra bootstrap method used in the first part of the demo.  

In both tests, we are not going to measure the Cassandra repair times, as you would expect them to not be different between scenarios.      

It should be noted that this lab is a test scenario designed to use in line steps to provison stroage be able to show specific steps and failover improvements.  It is not recommended using this demo confguration for production environments.  Its typical for large K8S clusters running in production to use PX to provision storage volumes dynamically in order to reduce administration burdens and complexities while increasing continous delievery speeds by incorporating automation.  



